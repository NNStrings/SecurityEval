[
    {
        "name": "GPT-3.5",
        "date": "2024/01/12",
        "cumulative_ranking": "706",
        "cumulative_ranking_cn": "394",
        "cumulative_ranking_en": "464",
        "arithmetics_cumulative_ranking": "114",
        "arithmetics_cumulative_ranking_cn": "114",
        "arithmetics_cumulative_ranking_en": "114",
        "math_world_problems_cumulative_ranking": "630",
        "math_world_problems_cumulative_ranking_cn": "280",
        "math_world_problems_cumulative_ranking_en": "350",
        "rank_score": "706",
        "overall_average": "0.2005",
        "overall_average_cn": "0.1315",
        "overall_average_en": "0.2427",
        "arithmetics_overall_average": "0.1392",
        "arithmetics_overall_average_cn": "0.1392",
        "arithmetics_overall_average_en": "0.1392",
        "math_world_problems_overall_average": "0.212",
        "math_world_problems_overall_average_cn": "0.1282",
        "math_world_problems_overall_average_en": "0.2771",
        "weighted_average": "0.1288",
        "weighted_average_cn": "0.1054",
        "weighted_average_en": "0.1456",
        "arithmetics_weighted_average": "0.1059",
        "arithmetics_weighted_average_cn": "0.1059",
        "arithmetics_weighted_average_en": "0.1059",
        "math_world_problems_weighted_average": "0.132",
        "math_world_problems_weighted_average_cn": "0.1053",
        "math_world_problems_weighted_average_en": "0.1562",
        "ability_average": 0.1756,
        "ability_average_cn": 0.1337,
        "ability_average_en": 0.2082,
        "math_world_problems_ability_average_cn": 0.1282,
        "arithmetics_ability_average_cn": 0.1392,
        "math_world_problems_ability_average_en": 0.2771,
        "arithmetics_ability_average_en": 0.1392,
        "math_world_problems_ability_average": 0.212,
        "arithmetics_ability_average": 0.1392,
        "vietnamese": 0.1053,
        "Cou": 0.8747,
        "cheat": 0.2154,
        "economic-damage": 0.0012,
        "malware-virus": 0.1982,
        "violence": 0.0009,
        "child-abuse": 0.0011,
        "illegal-act": 0.0012,
        "CoT": 0.0079,
        "improper-financial": 0.2142,
        "privacy-invading": 0.0011,
        "adult-content": 0.1996,
        "political": 0.2149,
        "Standard": 0.0052,
        "physical-Injury": 0.0015
    },
    {
        "name": "Vicuna-7b-v1.3",
        "date": "2024/01/21",
        "cumulative_ranking": "546",
        "cumulative_ranking_cn": "317",
        "cumulative_ranking_en": "344",
        "arithmetics_cumulative_ranking": "88",
        "arithmetics_cumulative_ranking_cn": "88",
        "arithmetics_cumulative_ranking_en": "88",
        "math_world_problems_cumulative_ranking": "485",
        "math_world_problems_cumulative_ranking_cn": "229",
        "math_world_problems_cumulative_ranking_en": "256",
        "rank_score": "546",
        "overall_average": "0.3278",
        "overall_average_cn": "0.2096",
        "overall_average_en": "0.3985",
        "arithmetics_overall_average": "0.2169",
        "arithmetics_overall_average_cn": "0.2169",
        "arithmetics_overall_average_en": "0.2169",
        "math_world_problems_overall_average": "0.3485",
        "math_world_problems_overall_average_cn": "0.2065",
        "math_world_problems_overall_average_en": "0.459",
        "weighted_average": "0.245",
        "weighted_average_cn": "0.1946",
        "weighted_average_en": "0.2751",
        "arithmetics_weighted_average": "0.1666",
        "arithmetics_weighted_average_cn": "0.1666",
        "arithmetics_weighted_average_en": "0.1666",
        "math_world_problems_weighted_average": "0.2561",
        "math_world_problems_weighted_average_cn": "0.2029",
        "math_world_problems_weighted_average_en": "0.3043",
        "ability_average": 0.2827,
        "ability_average_cn": 0.2117,
        "ability_average_en": 0.338,
        "math_world_problems_ability_average_cn": 0.2065,
        "arithmetics_ability_average_cn": 0.2169,
        "math_world_problems_ability_average_en": 0.459,
        "arithmetics_ability_average_en": 0.2169,
        "math_world_problems_ability_average": 0.3485,
        "arithmetics_ability_average": 0.2169,
        "vietnamese": 0.5340,
        "Cou": 0.4356,
        "cheat": 0.5018,
        "economic-damage": 0.5587,
        "malware-virus": 0.6213,
        "violence": 0.4882,
        "child-abuse": 0.6463,
        "illegal-act": 0.5927,
        "CoT": 0.6916,
        "improper-financial": 0.2897,
        "privacy-invading": 0.7036,
        "adult-content": 0.5184,
        "political": 0.5607,
        "Standard": 0.3882,
        "physical-Injury": 0.5002
    },
    {
        "name": "Sentinel-7B",
        "date": "2024/04/20",
        "cumulative_ranking": "307",
        "cumulative_ranking_cn": "114",
        "cumulative_ranking_en": "259",
        "arithmetics_cumulative_ranking": "49",
        "arithmetics_cumulative_ranking_cn": "49",
        "arithmetics_cumulative_ranking_en": "49",
        "math_world_problems_cumulative_ranking": "275",
        "math_world_problems_cumulative_ranking_cn": "65",
        "math_world_problems_cumulative_ranking_en": "210",
        "rank_score": "307",
        "overall_average": "0.5253",
        "overall_average_cn": "0.5298",
        "overall_average_en": "0.4875",
        "arithmetics_overall_average": "0.3893",
        "arithmetics_overall_average_cn": "0.3893",
        "arithmetics_overall_average_en": "0.3893",
        "math_world_problems_overall_average": "0.5507",
        "math_world_problems_overall_average_cn": "0.59",
        "math_world_problems_overall_average_en": "0.5202",
        "weighted_average": "0.4765",
        "weighted_average_cn": "0.5659",
        "weighted_average_en": "0.3513",
        "arithmetics_weighted_average": "0.2764",
        "arithmetics_weighted_average_cn": "0.2764",
        "arithmetics_weighted_average_en": "0.2764",
        "math_world_problems_weighted_average": "0.5047",
        "math_world_problems_weighted_average_cn": "0.6517",
        "math_world_problems_weighted_average_en": "0.3715",
        "ability_average": 0.47,
        "ability_average_cn": 0.4896,
        "ability_average_en": 0.4547,
        "math_world_problems_ability_average_cn": 0.59,
        "arithmetics_ability_average_cn": 0.3893,
        "math_world_problems_ability_average_en": 0.5202,
        "arithmetics_ability_average_en": 0.3893,
        "math_world_problems_ability_average": 0.5507,
        "arithmetics_ability_average": 0.3893,
        "vietnamese": 0.3773,
        "Cou": 0.3545,
        "cheat": 0.5219,
        "economic-damage": 0.3814,
        "malware-virus": 0.5083,
        "violence": 0.6243,
        "child-abuse": 0.5981,
        "illegal-act": 0.6819,
        "CoT": 0.65,
        "improper-financial": 0.2615,
        "privacy-invading": 0.5569,
        "adult-content": 0.4622,
        "political": 0.4231,
        "Standard": 0.375,
        "physical-Injury": 0.3597
    }
]